{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MJfnTHZxb_xz",
        "mgWHbm9edUvm",
        "PujxQ9zpdXfU",
        "Gm2k1PwQgKM-",
        "pNcgmbbBgOyt",
        "odEdHYddjbqt",
        "tX4VnwVpm2cw",
        "6WCkwP0PRMm7",
        "46D4CZWMRRPm",
        "kY8Pl5LORTDZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW2"
      ],
      "metadata": {
        "id": "MJfnTHZxb_xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 3"
      ],
      "metadata": {
        "id": "mgWHbm9edUvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 다음 정규식과 일치하는 문자열을 설명하십시오.\n",
        "\n",
        "a. [a-zA-Z]+ : 알파벳 1개 이상의 단어\n",
        "\n",
        "b. [A-Z][a-z]* : 대문자 1개로 시작하고 소문자 0개 이상의 단어\n",
        "\n",
        "c. p[aeiou]{,2}t : p로 시작하고, 중간에 소문자 모음 최대 2개, t로 끝나는 단어\n",
        "\n",
        "d. \\d+(.\\d+)? : 0 이상의 유리수\n",
        "\n",
        "e. ([^aeiou][aeiou][^aeiou])* : [소문자 모음을 제외한 글자-소문자 모음-소문자 모음을 제외한 글자]\n",
        "조합으로 0개 이상인 단어\n",
        "\n",
        "f. \\w+|[^\\w\\s]+ : 공백을 제외하고 글자 1개 이상의 단어"
      ],
      "metadata": {
        "id": "JnZOUbUCcHEV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLh0YtoLZ4-U"
      },
      "outputs": [],
      "source": [
        "# nltk.re_show를 사용하여 답변을 테스트하십시오.\n",
        "import nltk, re\n",
        "nltk.re_show(r'[a-zA-Z]+', 'Regular expressions 1234 a1B')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.re_show(r'[A-Z][a-z]*', 'Regular expressions 1234 a1B')"
      ],
      "metadata": {
        "id": "_J1Ii8D8cUAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.re_show(r'\\d+(.\\d+)?', '0.1234 22 -3 -1.12 1/3 -5/1')"
      ],
      "metadata": {
        "id": "_QqtxONAcU1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.re_show(r'p[aeiou]{,2}t', 'pt pait paiut pAt')"
      ],
      "metadata": {
        "id": "7D_0vA2UcY4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', \"bab babbab bbb\")"
      ],
      "metadata": {
        "id": "7JlMuSX5cagF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.re_show(r'\\w+|[^\\w\\s]', \"Regular expressions 1234 a1B_\")"
      ],
      "metadata": {
        "id": "5t3kdTmNcbbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. URL을 인수로 사용하고 해당 웹 페이지에서 발생하는 알 수 없는 단어 목록을 반환하는 함수 unknown()을 작성합니다. 이렇게 하려면 소문자로 구성된 모든 하위 문자열을 추출하고(re.findall() 사용) WordsCorpus(nltk.corpus.words)에서 발생하는 항목을 이 집합에서 제거합니다. 이 단어들을 수동으로 분류하고 결과에 대해 논의해 보십시오."
      ],
      "metadata": {
        "id": "v4vAsMyccmeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk import word_tokenize\n",
        "from urllib import request\n",
        "import re\n",
        "nltk.download('words')\n",
        "def unknown(url):\n",
        "  response = request.urlopen(url)\n",
        "  raw = response.read().decode('utf8')\n",
        "  lowercase_letters = re.findall(r'[a-z]+', raw)\n",
        "  wordlist = set(words.words())\n",
        "  unknowns = [word for word in lowercase_letters if word not in wordlist]\n",
        "  return unknowns\n",
        "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n",
        "print(unknown(url))"
      ],
      "metadata": {
        "id": "6qe-7beMcdVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Porter Stemmer를 사용하여 토큰화된 텍스트를 정규화하고 각각의 Stemmer를 호출합니다. Lancaster Stemmer도 똑같이 해보고 차이점이 있는지 확인해보세요."
      ],
      "metadata": {
        "id": "8DfxnXDVdDXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "raw = \"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system Supreme executive power derives from a mandate from the masses, not from some farcical aquatic cere\"\n",
        "tokens = word_tokenize(raw)\n",
        "print(tokens)\n",
        "print([nltk.PorterStemmer().stem(t) for t in tokens])\n",
        "print([nltk.LancasterStemmer().stem(t) for t in tokens])"
      ],
      "metadata": {
        "id": "ciFt7zpydGDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 5"
      ],
      "metadata": {
        "id": "PujxQ9zpdXfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Brown Corpus 를 처리하기 위한 프로그램을 작성하고 다음 질문에 대한 답을 찾으십시오."
      ],
      "metadata": {
        "id": "A32mfWP1dY4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.어떤 명사가 단수형보다 복수형이 더 일반적인가요? (-s 접미사로 형성된 복수형만 고려하십시오.)\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "# create set of all singular nouns in the Brown Corpus\n",
        "sing_nouns = set(word.lower() for (word, tag) in nltk.corpus.brown.tagged_words() if tag == 'NN')\n",
        "# create set of all plural nouns in the Brown Corpus\n",
        "plur_nouns = set([word.lower() for (word, tag) in nltk.corpus.brown.tagged_words() if tag == 'NNS'])\n",
        "# make a list of words that have regular plurals represented in plur_nouns\n",
        "cands = [n for n in sing_nouns if n + \"s\" in plur_nouns]\n",
        "# get FreqDist for singular nouns\n",
        "snfd = nltk.FreqDist(word.lower() for (word, _) in nltk.corpus.brown.tagged_words() if word in sing_nouns)\n",
        "# get FreqDist for plural nouns\n",
        "pnfd = nltk.FreqDist(word.lower() for (word, _) in nltk.corpus.brown.tagged_words() if word in plur_nouns)\n",
        "# find out which words are more common in the plural form\n",
        "more_common_plur = [(pnfd[c + 's'], c + 's', snfd[c], c) for c in cands if pnfd[c + 's'] > snfd[c]]\n",
        "sorted(more_common_plur, reverse = True)[:20]"
      ],
      "metadata": {
        "id": "ffcs_5L3dP56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.구별되는 태그의 수가 가장 많은 단어는 무엇입니까? 그것들은 무엇이며, 무엇을 대표하는가?\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from collections import defaultdict\n",
        "nltk.download('brown')\n",
        "nltk.download('tagsets')\n",
        "# create a ConditionalFreqDist with all the words in the corpus,\n",
        "# with counts of each of their tags\n",
        "dt = nltk.ConditionalFreqDist(brown.tagged_words())\n",
        "# create a dictionary where the keys will be the number of distinct tags\n",
        "tags = defaultdict(list)\n",
        "for w in set(brown.words()):\n",
        "    tags[len(dt[w])].append(w)\n",
        "# word with the most tags\n",
        "print(max(tags))\n",
        "print(tags[12])\n",
        "for tag in dt['that']:\n",
        "    nltk.help.brown_tagset(tag)"
      ],
      "metadata": {
        "id": "zvhnMPvBdelx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 태그를 빈도가 낮은 순서대로 나열합니다. 가장 빈번한 20개의 태그는 무엇을 나타냅니까?\n",
        "tags = [tags for _, tags in brown.tagged_words()]\n",
        "ft = nltk.FreqDist(tags)\n",
        "print(ft.most_common(100), end = '')\n",
        "for tag, _ in ft.most_common(20):\n",
        "    nltk.help.brown_tagset(tag)"
      ],
      "metadata": {
        "id": "px6OIE_adwVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 명사 다음에 가장 흔하게 발견되는 태그는 무엇인가? 이 태그들은 무엇을 나타냅니까?\n",
        "wtp = nltk.bigrams(brown.tagged_words())\n",
        "np = [a[1] for (a, b) in wtp if b[1].startswith('NN')]\n",
        "fd = nltk.FreqDist(np)\n",
        "prec_tags = [tag for (tag, _) in fd.most_common(20)]\n",
        "print(prec_tags, end = '')\n",
        "for tag in prec_tags:\n",
        "    nltk.help.brown_tagset(tag)"
      ],
      "metadata": {
        "id": "_prrRu5Od4dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Brown Corpus 에서 태그에 따라 특정 단어와 구문을 검색하는 코드를 작성합니다.\n",
        "다음 질문에 답합니다."
      ],
      "metadata": {
        "id": "0tSmOcK3eHtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a. MD로 태그된 고유 단어의 알파벳 순으로 정렬된 목록을 생성합니다.\n",
        "print(sorted(set([w.lower() for w, t in brown.tagged_words() if t == 'MD'])), end = '')"
      ],
      "metadata": {
        "id": "mMC0r9nEd6x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b. 복수 명사 또는 3인칭 단수 동사(예:deals, flies)가 될 수 있는 단어를 식별합니다.\n",
        "cfd = nltk.ConditionalFreqDist(brown.tagged_words())\n",
        "NNS_VBZ = set([w for w in set(w.lower() for w, t in brown.tagged_words()) if 'NNS' in cfd[w] and 'VBZ'])\n",
        "print(NNS_VBZ, end = '')"
      ],
      "metadata": {
        "id": "lan3nmuBeMPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c. IN + DET + NN 형식의 세 단어 전치사 구문을 식별합니다(예:in the lab)\n",
        "three_word_pp = [(w1, w2, w3) for sent in brown.tagged_sents() for (w1, t1), (w2, t2), (w3, t3) in nltk.trigrams(sent) if (t1=='IN' and t2 == 'DT' and t3.startswith('NN'))]\n",
        "print(three_word_pp[:50], end = '')"
      ],
      "metadata": {
        "id": "qXidR1X3gpMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d. 남성 대명사와 여성 대명사의 비율은 어떻게 되나요?\n",
        "print([set(w.lower() for w,t in brown.tagged_words() if t.startswith('PP'))])\n",
        "m = [\"he'd\",  \"he'll\",  \"he's\",  \"'im\",  'he',  'him',  'himself',  'himselfe',  'his',  'hisself', 'hym', 'hys', \"h'all\"]\n",
        "f = [\"she'd\",  \"she'll\",  \"she's\",  'her',  'hers',  'herself',  'hir',  'she']\n",
        "m_values = sum([sum(cfd[mp].values()) for mp in m])\n",
        "f_values = sum([sum(cfd[fp].values()) for fp in f])\n",
        "print(m_values/f_values)"
      ],
      "metadata": {
        "id": "6TPtTghben39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW3"
      ],
      "metadata": {
        "id": "Gm2k1PwQgKM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chapter 7"
      ],
      "metadata": {
        "id": "pNcgmbbBgOyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. IOB 형식은 태그가 지정된 토큰을 I, O 및 B로 분류합니다. 왜 세 개의 태그가 필요한가요? I 태그와 O 태그만 사용하면 어떤 문제가 발생할까요?\n",
        "\n",
        "I 태그와 O 태그만 사용할 경우 발생할 수 있는 한 가지 문제는 다음 작업이 불가능하다는 것입니다.\n",
        "하나의 긴 덩어리를 서로 바로 인접한 두 개의 작은 덩어리와 구별한다.\n",
        "경계를 식별하는 태그가 없을 것이기 때문입니다.\n"
      ],
      "metadata": {
        "id": "9d6ozTrNhUUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 동명사를 포함하는 명사구를 포함하는 태그 패턴을 작성합니다(예:\"the/DT receiving/VBG\n",
        "end/NN\", \"assistant/NN managing/VBG editor/NN\"). 이 패턴들을 문법에 추가하세요.\n",
        "라인. 당신 자신의 장치의 태그가 달린 문장을 사용하여 당신의 작업을 테스트하세요.\n"
      ],
      "metadata": {
        "id": "js5SKOpRh_bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "grammar = \"\"\"\n",
        "    NP: {<DT>?<VBG>*<NN>}\n",
        "\"\"\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "sentences = [[(\"the\", \"DT\"), (\"receiving\", \"VBG\"), (\"end\", \"NN\")], \n",
        "             [(\"assistant\", \"NN\"),  (\"managing\", \"VBG\"),  (\"editor\", \"NN\")]]\n",
        "for sent in sentences:\n",
        "    print(cp.parse(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB901RFmhTAP",
        "outputId": "b63bc233-ba1c-4af7-ee2a-5495c1c45462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP the/DT receiving/VBG end/NN))\n",
            "(S (NP assistant/NN) (NP managing/VBG editor/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음과 같은 성능 메트릭의 의미를 설명합니다.\n",
        "Accuracy, Precision, Recall, F-measure which are\n",
        "generated by \"accuracy\" function\n",
        "\n",
        "- Accuracy : 올바르게 예측된 데이터의 수를 전체 데이터의 수로 나눈 값\n",
        "- Precision : 모델이 True로 예측한 데이터 중 실제로 True인 데이터 수\n",
        "- Recall : 실제로 True인 데이터를 모델이 True라고 인식한 데이터 수\n",
        "- F-measure : precision 과 recall의 조화평균"
      ],
      "metadata": {
        "id": "pUPu_5AeiEP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"UnigramChunker\"와 \"BigramChunker\"를 조사하고, 왜 \"BigramChunker\" 가 더 좋은 성능을 내는지 말하세요.\n",
        "\n",
        "- n-gram은 n개의 연속적인 단어 나열을 의미한다. n개의 단어 단위로 끊어 하나의 토큰으로 간주한다\n",
        "\n",
        "유니그램(unigram) : n이 1일 때, 1개의 단어를 하나의 토큰으로 간주한다.\n",
        "\n",
        "바이그램(bigram) : n이 2일 때, 2개의 단어를 하나의 토큰으로 간주한다.\n",
        "\n",
        "문장의 맥락을 파악하기 위해서는 1개의 단어를 사용하는것 보다 2개의 단어를 함께 사용하는게 더 좋다.\n"
      ],
      "metadata": {
        "id": "XNnshHu5iGbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 8"
      ],
      "metadata": {
        "id": "odEdHYddjbqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Kim arrived or Dana left and everyone cheered 문장을 생각해보라. 괄호로 묶은 양식을 작성하여 및 또는 의 상대적 범위를 표시합니다. 이 두 해석에 해당하는 트리 구조를 생성합니다."
      ],
      "metadata": {
        "id": "rAKmE5cYiRU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhmIXAKZiXFi",
        "outputId": "2594d4df-3c07-444a-a87a-93403677c42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.3.1 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import Tree\n",
        "string = '(S (S (S (NP (N Kim)) (VP (V arrived))) (CC or) (S (NP (N Dana)) (VP (V left)))) (CC and) (S (NP (N everyone)) (VP (V cheered))))'\n",
        "nltk.Tree.fromstring(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "j_azb1AqiYNw",
        "outputId": "1ab4d6fc-3af9-4856-d8fc-6ad3bbc58848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('S', [Tree('S', [Tree('NP', [Tree('N', ['Kim'])]), Tree('VP', [Tree('V', ['arrived'])])]), Tree('CC', ['or']), Tree('S', [Tree('NP', [Tree('N', ['Dana'])]), Tree('VP', [Tree('V', ['left'])])])]), Tree('CC', ['and']), Tree('S', [Tree('NP', [Tree('N', ['everyone'])]), Tree('VP', [Tree('V', ['cheered'])])])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"264px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,432.0,264.0\" width=\"432px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"55.5556%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"46.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"35.7143%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">N</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Kim</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"64.2857%\" x=\"35.7143%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">arrived</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.8571%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.3333%\" x=\"46.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CC</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">or</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">N</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Dana</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">left</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.7778%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.25926%\" x=\"55.5556%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CC</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.1852%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35.1852%\" x=\"64.8148%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"52.6316%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">N</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">everyone</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.3158%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.3684%\" x=\"52.6316%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cheered</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.3158%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.4074%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. a) parse trees를 수동으로 작성해보자. Write code to produce two trees, one for each reading of the phrase old men and women"
      ],
      "metadata": {
        "id": "D21c0fyajJ-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP CC NP | JJ NP\n",
        "NP -> N | JJ N | N CC N\n",
        "N -> 'men' | 'women'\n",
        "JJ -> 'old'\n",
        "CC -> 'and'\n",
        "\"\"\")\n",
        "sent = ['old', 'men', 'and', 'women']\n",
        "parser = nltk.ChartParser(grammar)\n",
        "for tree in parser.parse(sent):\n",
        "    print(tree)\n",
        "trees = []\n",
        "for tree in parser.parse(sent):\n",
        "    trees.append(str(tree))\n",
        "nltk.Tree.fromstring(trees[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "-dlXXopHjIZ9",
        "outputId": "d0742335-dc56-49c3-ee03-2966730201ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (JJ old) (NP (N men) (CC and) (N women)))\n",
            "(S (NP (JJ old) (N men)) (CC and) (NP (N women)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('JJ', ['old']), Tree('NP', [Tree('N', ['men']), Tree('CC', ['and']), Tree('N', ['women'])])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,176.0,168.0\" width=\"176px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">old</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"77.2727%\" x=\"22.7273%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">N</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">men</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"29.4118%\" x=\"29.4118%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CC</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.1176%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"41.1765%\" x=\"58.8235%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">N</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">women</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.4118%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.3636%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 9"
      ],
      "metadata": {
        "id": "tX4VnwVpm2cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "sentence = [(\"the\",\"DT\"),(\"IittIe\",\"JJ\"), (\"yelIow\",\"JJ\"), (\"dog\",\"NN\"), (\"barked\",\"VBD\"), (\"at\",\"IN\"), (\"the\",\"DT\"), (\"cat\", \"NN\")]\n",
        "grammar = \"NP : {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result= cp.parse(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoEsOmkmGjUg",
        "outputId": "df714f69-a507-415e-d8da-b5828754fdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT IittIe/JJ yelIow/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT cat/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = r\"\"\"\n",
        "  NP: {<DT|PP\\$>?<JJ>*<NN>}     # chunk determiner/possessive, adjectives and noun\n",
        "      {<NNP>+}                  # chunk sequences of proper nouns\n",
        "\"\"\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\",\"VBD\"), (\"down\",\"RP\"),\n",
        "            (\"her\",\"PP$\"), (\"long\",\"JJ\"), (\"goIden\",\"JJ\"), (\"hair\",\"NN\")]\n",
        "print(cp.parse(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGg0Eu2-Hm46",
        "outputId": "f8e1cd24-8c2f-4b0b-d4ad-a7a0f0e10086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP Rapunzel/NNP)\n",
            "  let/VBD\n",
            "  down/RP\n",
            "  (NP her/PP$ long/JJ goIden/JJ hair/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nouns = [(\"money\",\"NN\"), (\"market\",\"NN\"), (\"fund\",\"NN\") ]\n",
        "grammar = \"NP: {<NN><NN>}   # Chunk two consecutive nouns\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.parse(nouns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AleexTbjIZfh",
        "outputId": "7e49d498-f1bb-4e8c-9872-c9ef97596139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP money/NN market/NN) fund/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = r\"\"\"\n",
        "  NP:\n",
        "    {<.*>+}             # Chunk everything\n",
        "    }<VBD|IN>+{         # Chink sequences of VBD and IN\n",
        "\"\"\"\n",
        "sentence= [(\"the\",\"DT\"), (\"little\",\"JJ\"), (\"yellow\",\"JJ\"),\n",
        "           (\"dog\",\"NN\"), (\"barked\",\"VBD\"), (\"at\",\"IN\"), (\"the\",\"DT\"), (\"cat\",\"NN\")]\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.parse(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_srZpmN2JT6j",
        "outputId": "715e5fcf-0673-46e8-8ed4-e607f1aba258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT cat/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2000\n",
        "print(conll2000.chunked_sents('train.txt')[99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkVbGe3LKM2e",
        "outputId": "56b2c6e6-120e-42e0-ff36-e26543090ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PP Over/IN)\n",
            "  (NP a/DT cup/NN)\n",
            "  (PP of/IN)\n",
            "  (NP coffee/NN)\n",
            "  ,/,\n",
            "  (NP Mr./NNP Stone/NNP)\n",
            "  (VP told/VBD)\n",
            "  (NP his/PRP$ story/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conll2000.chunked_sents('train.txt', chunk_types= ['NP'])[99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FvptjBBKVge",
        "outputId": "71301d35-22a3-4079-b0d4-0fadcd6a2e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Over/IN\n",
            "  (NP a/DT cup/NN)\n",
            "  of/IN\n",
            "  (NP coffee/NN)\n",
            "  ,/,\n",
            "  (NP Mr./NNP Stone/NNP)\n",
            "  told/VBD\n",
            "  (NP his/PRP$ story/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2000\n",
        "cp = nltk.RegexpParser(\"\")\n",
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "print(cp.accuracy(test_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbzh3p_bLt3u",
        "outputId": "74584a86-defe-43c1-bb9d-f7d97130422f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  43.4%%\n",
            "    Precision:      0.0%%\n",
            "    Recall:         0.0%%\n",
            "    F-Measure:      0.0%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar= r\"NP: {<[CDJNP].*>+}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.accuracy(test_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEP0fFGYPjrX",
        "outputId": "fd302127-0f37-4ad2-9174-4b4a33ac13a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  87.7%%\n",
            "    Precision:     70.6%%\n",
            "    Recall:        67.8%%\n",
            "    F-Measure:     69.2%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramChunker(nltk.ChunkParserI):\n",
        "  def __init__(self, train_sents ):\n",
        "    train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
        "                  for sent in train_sents]\n",
        "    self.tagger = nltk.UnigramTagger(train_data)\n",
        "\n",
        "  def parse(self, sentence):\n",
        "    pos_tags = [pos for (word,pos) in sentence]\n",
        "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
        "    conlltags= [(word, pos, chunktag) for ((word,pos),chunktag)\n",
        "                in zip(sentence, chunktags)]\n",
        "    return nltk.chunk.conlltags2tree(conlltags)"
      ],
      "metadata": {
        "id": "GtX9hZcrPt8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
        "unigram_chunker = UnigramChunker(train_sents)\n",
        "print(unigram_chunker.accuracy(test_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWY2TM7lQfSS",
        "outputId": "40957a23-71b7-4514-8fd8-b78e5347433c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  92.9%%\n",
            "    Precision:     79.9%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     83.2%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 10"
      ],
      "metadata": {
        "id": "6WCkwP0PRMm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('conll2000')\n",
        "from nltk.corpus import conll2000\n",
        "print(conll2000.chunked_sents('train.txt')[99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFsSyVtTRWkS",
        "outputId": "1d7be173-1d4b-44ad-af44-efe3c1e72d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PP Over/IN)\n",
            "  (NP a/DT cup/NN)\n",
            "  (PP of/IN)\n",
            "  (NP coffee/NN)\n",
            "  ,/,\n",
            "  (NP Mr./NNP Stone/NNP)\n",
            "  (VP told/VBD)\n",
            "  (NP his/PRP$ story/NN)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3177qW0uRs9m",
        "outputId": "f4655624-dacb-4c75-e66f-66ce418a957c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Over/IN\n",
            "  (NP a/DT cup/NN)\n",
            "  of/IN\n",
            "  (NP coffee/NN)\n",
            "  ,/,\n",
            "  (NP Mr./NNP Stone/NNP)\n",
            "  told/VBD\n",
            "  (NP his/PRP$ story/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2000\n",
        "cp = nltk.RegexpParser(\"\")\n",
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "print(cp.accuracy(test_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZdmbGRCR7mp",
        "outputId": "bac3543d-2693-4f18-d947-68f5362de867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  43.4%%\n",
            "    Precision:      0.0%%\n",
            "    Recall:         0.0%%\n",
            "    F-Measure:      0.0%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar= r\"NP: {<[CDJNP].*>+}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.accuracy(test_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzCeqD4RSRlS",
        "outputId": "1ab94029-08f6-49eb-8846-29e0da4aa664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  87.7%%\n",
            "    Precision:     70.6%%\n",
            "    Recall:        67.8%%\n",
            "    F-Measure:     69.2%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramChunker(nltk.ChunkParserI):\n",
        "  def __init__(self, train_sents ):\n",
        "    train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
        "                  for sent in train_sents]\n",
        "    self.tagger = nltk.UnigramTagger(train_data)\n",
        "\n",
        "  def parse(self, sentence):\n",
        "    pos_tags = [pos for (word,pos) in sentence]\n",
        "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
        "    conlltags= [(word, pos, chunktag) for ((word,pos),chunktag)\n",
        "                in zip(sentence, chunktags)]\n",
        "    return nltk.chunk.conlltags2tree(conlltags)"
      ],
      "metadata": {
        "id": "8qQrkCs5SZCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
        "unigram_chunker = UnigramChunker(train_sents)\n",
        "print(unigram_chunker.accuracy(test_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLI06XyMStA-",
        "outputId": "a695857d-c53f-4e56-e912-74bbec13df93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  92.9%%\n",
            "    Precision:     79.9%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     83.2%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}             # Chunk sequences of DT, JJ, NN\n",
        "  PP: {<IN><NP>}                  # Chunk prepositions followed by NP\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>+$}    # Chunk verbs and their arguments\n",
        "  CLAUSE: {<NP><VP>}              # Chunk NP, VP\n",
        "  \"\"\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "sentence = [(\"Mary\",\"NN\"), (\"saw\",\"VBD\"), (\"the\",\"DT\"), (\"cat\",\"NN\"),\n",
        "            (\"sit\",\"VB\"), (\"on\",\"IN\"), (\"the\",\"DT\"), (\"mat\", \"NN\")]\n",
        "print(cp.parse(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pao_-imSzSC",
        "outputId": "62cdc9ff-a12a-4045-aaeb-4c13d4db45e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP Mary/NN)\n",
            "  saw/VBD\n",
            "  (CLAUSE\n",
            "    (NP the/DT cat/NN)\n",
            "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [(\"John\",\"NNP\"), (\"thinks\",\"VBZ\"), (\"Mary\",\"NN\"),\n",
        "            (\"saw\",\"VBD\"), (\"the\",\"DT\"), (\"cat\",\"NN\"), (\"sit\",\"VB\"),\n",
        "            (\"on\",\"IN\"), (\"the\",\"DT\"), (\"mat\",\"NN\")]\n",
        "print(cp.parse(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeNScvvsUXxs",
        "outputId": "cac2f44d-7d21-44c4-e5a1-ef00c039a017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP John/NNP)\n",
            "  thinks/VBZ\n",
            "  (NP Mary/NN)\n",
            "  saw/VBD\n",
            "  (CLAUSE\n",
            "    (NP the/DT cat/NN)\n",
            "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp = nltk.RegexpParser(grammar, loop=2)\n",
        "print(cp.parse(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlUs11HTUYJs",
        "outputId": "1518fca4-0d14-4197-f182-a74c7824ad39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP John/NNP)\n",
            "  thinks/VBZ\n",
            "  (CLAUSE\n",
            "    (NP Mary/NN)\n",
            "    (VP\n",
            "      saw/VBD\n",
            "      (CLAUSE\n",
            "        (NP the/DT cat/NN)\n",
            "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
        "print(nltk.ne_chunk(sent, binary=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUVcQyBTVTkf",
        "outputId": "aed2619d-ccb3-42a7-8f76-76507213e6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (NE U.S./NNP)\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  few/JJ\n",
            "  industrialized/VBN\n",
            "  nations/NNS\n",
            "  that/WDT\n",
            "  *T*-7/-NONE-\n",
            "  does/VBZ\n",
            "  n't/RB\n",
            "  have/VB\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  standard/NN\n",
            "  of/IN\n",
            "  regulation/NN\n",
            "  for/IN\n",
            "  the/DT\n",
            "  smooth/JJ\n",
            "  ,/,\n",
            "  needle-like/JJ\n",
            "  fibers/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  crocidolite/NN\n",
            "  that/WDT\n",
            "  *T*-1/-NONE-\n",
            "  are/VBP\n",
            "  classified/VBN\n",
            "  *-5/-NONE-\n",
            "  as/IN\n",
            "  amphobiles/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (NE Brooke/NNP)\n",
            "  T./NNP\n",
            "  Mossman/NNP\n",
            "  ,/,\n",
            "  a/DT\n",
            "  professor/NN\n",
            "  of/IN\n",
            "  pathlogy/NN\n",
            "  at/IN\n",
            "  the/DT\n",
            "  (NE University/NNP)\n",
            "  of/IN\n",
            "  (NE Vermont/NNP College/NNP)\n",
            "  of/IN\n",
            "  (NE Medicine/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.ne_chunk(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6tE0WbAVwI5",
        "outputId": "5bceba46-30e2-4dd1-890e-8d233aafed8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (GPE U.S./NNP)\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  few/JJ\n",
            "  industrialized/VBN\n",
            "  nations/NNS\n",
            "  that/WDT\n",
            "  *T*-7/-NONE-\n",
            "  does/VBZ\n",
            "  n't/RB\n",
            "  have/VB\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  standard/NN\n",
            "  of/IN\n",
            "  regulation/NN\n",
            "  for/IN\n",
            "  the/DT\n",
            "  smooth/JJ\n",
            "  ,/,\n",
            "  needle-like/JJ\n",
            "  fibers/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  crocidolite/NN\n",
            "  that/WDT\n",
            "  *T*-1/-NONE-\n",
            "  are/VBP\n",
            "  classified/VBN\n",
            "  *-5/-NONE-\n",
            "  as/IN\n",
            "  amphobiles/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
            "  ,/,\n",
            "  a/DT\n",
            "  professor/NN\n",
            "  of/IN\n",
            "  pathlogy/NN\n",
            "  at/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION University/NNP)\n",
            "  of/IN\n",
            "  (PERSON Vermont/NNP College/NNP)\n",
            "  of/IN\n",
            "  (GPE Medicine/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download ('ieer')\n",
        "import re\n",
        "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
        "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
        "  for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
        "                                   corpus='ieer' , pattern = IN):\n",
        "    print(nltk.sem.rtuple(rel))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSWY5yYHV4tv",
        "outputId": "06277260-db90-440c-c16e-afe83fbb1c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
            "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
            "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
            "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
            "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
            "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
            "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
            "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
            "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
            "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
            "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
            "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
            "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]   Package ieer is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 11"
      ],
      "metadata": {
        "id": "46D4CZWMRRPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "groucho_grammar = nltk.CFG.fromstring (\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'an' | 'my'\n",
        "N -> 'elephant' | 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\n",
        "\"\"\")\n",
        "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        "parser= nltk.ChartParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7shrbjuCWlk7",
        "outputId": "d09e0eac-3248-4aea-db4c-1280c07a77c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V shot) (NP (Det an) (N elephant)))\n",
            "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V shot)\n",
            "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "grammar1 = nltk.CFG.fromstring (\"\"\"\n",
        "S -> NP VP\n",
        "VP -> V NP | V NP PP\n",
        "PP -> P NP\n",
        "V -> \"saw\" | \"ate\" | \"walked\"\n",
        "NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
        "Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        "N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        "P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        "\"\"\")\n",
        "sent = \"Mary saw Bob\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "for tree in rd_parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWwSAdiUXX4W",
        "outputId": "251d4a1f-7ea4-4da3-8421-5d0f77420b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 12"
      ],
      "metadata": {
        "id": "kY8Pl5LORTDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "grammar1 = nltk.CFG.fromstring (\"\"\"\n",
        "S -> NP VP\n",
        "VP -> V NP | V NP PP\n",
        "PP -> P NP\n",
        "V -> \"saw\" | \"ate\" | \"walked\"\n",
        "NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
        "Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        "N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        "P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        "\"\"\")\n",
        "sent = \"Mary saw Bob\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "for tree in rd_parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3sl7TJLYloP",
        "outputId": "407df39a-bc54-4b2a-9edf-6d2d1ec74df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
        "sent= 'Mary saw a dog' .split()\n",
        "for tree in sr_parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZlm23t0YtWP",
        "outputId": "854a26ab-62bf-4e5a-f97b-db38add4323e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groucho_dep_grammar = nltk.DependencyGrammar.fromstring (\"\"\"\n",
        "'shot' -> 'I' | 'elephant' | 'in'\n",
        "'elephant' -> 'an' | 'in'\n",
        "'in' -> 'pajamas'\n",
        "'pajamas' -> 'my'\n",
        "\"\"\" )\n",
        "print(groucho_dep_grammar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayLl1w2VY9fv",
        "outputId": "04e42ec1-f791-47e9-d455-4285e9240ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency grammar with 7 productions\n",
            "  'shot' -> 'I'\n",
            "  'shot' -> 'elephant'\n",
            "  'shot' -> 'in'\n",
            "  'elephant' -> 'an'\n",
            "  'elephant' -> 'in'\n",
            "  'in' -> 'pajamas'\n",
            "  'pajamas' -> 'my'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdp = nltk.ProjectiveDependencyParser (groucho_dep_grammar)\n",
        "sent= 'I shot an elephant in my pajamas'.split()\n",
        "trees = pdp.parse(sent)\n",
        "for tree in trees :\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz8aGjJUZSkB",
        "outputId": "0479254b-c267-4096-faff-5872fe90343b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(shot I (elephant an (in (pajamas my))))\n",
            "(shot I (elephant an) (in (pajamas my)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 13"
      ],
      "metadata": {
        "id": "v3D8uUstRT-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 14"
      ],
      "metadata": {
        "id": "jO-X6NI7RVAh"
      }
    }
  ]
}