{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import argparse\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
      "         MaxPool2d-2             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-3             [-1, 64, 7, 7]             128\n",
      "              ReLU-4             [-1, 64, 7, 7]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 7, 7]             128\n",
      "              ReLU-7             [-1, 64, 7, 7]               0\n",
      "            Conv2d-8             [-1, 64, 7, 7]          36,864\n",
      "    Residual_Block-9             [-1, 64, 7, 7]               0\n",
      "      BatchNorm2d-10             [-1, 64, 7, 7]             128\n",
      "             ReLU-11             [-1, 64, 7, 7]               0\n",
      "           Conv2d-12             [-1, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 7, 7]             128\n",
      "             ReLU-14             [-1, 64, 7, 7]               0\n",
      "           Conv2d-15             [-1, 64, 7, 7]          36,864\n",
      "   Residual_Block-16             [-1, 64, 7, 7]               0\n",
      "      BatchNorm2d-17             [-1, 64, 7, 7]             128\n",
      "           Conv2d-18            [-1, 256, 7, 7]         147,712\n",
      "      BatchNorm2d-19            [-1, 256, 7, 7]             512\n",
      "             ReLU-20            [-1, 256, 7, 7]               0\n",
      "           Conv2d-21            [-1, 256, 7, 7]         589,824\n",
      "      BatchNorm2d-22            [-1, 256, 7, 7]             512\n",
      "             ReLU-23            [-1, 256, 7, 7]               0\n",
      "           Conv2d-24            [-1, 256, 7, 7]         589,824\n",
      "   Residual_Block-25            [-1, 256, 7, 7]               0\n",
      "      BatchNorm2d-26            [-1, 256, 7, 7]             512\n",
      "             ReLU-27            [-1, 256, 7, 7]               0\n",
      "           Conv2d-28            [-1, 256, 7, 7]         589,824\n",
      "      BatchNorm2d-29            [-1, 256, 7, 7]             512\n",
      "             ReLU-30            [-1, 256, 7, 7]               0\n",
      "           Conv2d-31            [-1, 256, 7, 7]         589,824\n",
      "   Residual_Block-32            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-33            [-1, 256, 1, 1]               0\n",
      "             View-34                  [-1, 256]               0\n",
      "           Linear-35                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 2,662,922\n",
      "Trainable params: 2,662,922\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.92\n",
      "Params size (MB): 10.16\n",
      "Estimated Total Size (MB): 12.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class View(nn.Module):\n",
    "    \n",
    "    def __init__(self, *shape): \n",
    "        super(View, self).__init__() \n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], *self.shape) # x.shape = [batch_size, channel, width, height]\n",
    "\n",
    "class Residual_Block(nn.Module):  # Residual Block 만들기 \n",
    "    \n",
    "    def __init__(self, n_ch): # pre activation 적용한 것 (2번째 논문 5번 그림)\n",
    "        super(Residual_Block, self).__init__() \n",
    "        layers = []\n",
    "        layers += [nn.BatchNorm2d(num_features=n_ch),\n",
    "                  nn.ReLU(inplace=True), \n",
    "                  nn.Conv2d(in_channels=n_ch, out_channels=n_ch, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                  nn.BatchNorm2d(num_features=n_ch),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  nn.Conv2d(in_channels=n_ch, out_channels=n_ch, kernel_size=3, stride=1, padding=1, bias=False)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layers(x)\n",
    "        return x + out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # mnist니까 in_channels=1, 논문에서 out_channels=64, kernel_size=7, stride=2 (논문에선 사이즈 줄이기 위해 - 28*28이 14*14로)\n",
    "        # 사이즈 안 줄게 하는 padding 공식 = (kernel_size-1)/2\n",
    "        \n",
    "        # 우리는 Residual_Block 2개씩만 하고 (논문은 3개씩), in_channel은 64랑 256만.\n",
    "        \n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3), # batch * 64 * 14 * 14\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1), # batch * 64 * 7 * 7\n",
    "                   Residual_Block(n_ch=64),\n",
    "                   Residual_Block(n_ch=64),\n",
    "                   nn.BatchNorm2d(64), # Residual_Block 거치면 x가 새롭게 추가 되니까 그것에 대해서도 BN (논문에선 안함)\n",
    "                   nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1),\n",
    "                   Residual_Block(n_ch=256),\n",
    "                   Residual_Block(n_ch=256), # batch * 256 * 7 * 7\n",
    "                   nn.AdaptiveAvgPool2d((1,1)), # batch * 256 * 1 * 1: (1,1)은 입력되는 커널 사이즈가 아니라 그렇게 나가도록 하라는 것\n",
    "                   View(-1), # batch * 256 * 1 * 1 는 우리가 보기에 1차원이지만 컴퓨터가 보기엔 4차원이므로 그걸 해결하기 위함\n",
    "                   nn.Linear(in_features=256, out_features=10)]\n",
    "                      \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "if __name__ == '__main__': # 이 파일이 메인으로 돌아갈때만 아래를 실행한다\n",
    "    model = ResNet()\n",
    "    summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터에 대해서 훈련 및 검증\n",
    "\n",
    "# initial parameters\n",
    "\n",
    "seed = 6 # weight 초깃값 설정: 이렇게 설정해주면 돌릴때마다 weight 안변함\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args('')\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.batch_size = 256\n",
    "args.model = ResNet()\n",
    "args.loss_fn = nn.CrossEntropyLoss()\n",
    "args.batch_size = 1024\n",
    "args.lr = 0.01\n",
    "args.epoch = 5\n",
    "\n",
    "# 데이터 준비\n",
    "\n",
    "train_datasets = MNIST(root='./datasets', train=True, transform=ToTensor(), download=True)\n",
    "validation_datasets = MNIST(root='./datasets', train=False, transform=ToTensor(), download=True)\n",
    "\n",
    "def acc(y_pred, y_true, batch_size):\n",
    "    accuracy = torch.sum(torch.eq(torch.argmax(y_pred, dim=1), y_true)).item() / batch_size * 100 \n",
    "    return accuracy\n",
    "\n",
    "def train(model, datasets, optimizer, loss_fn, args):\n",
    "    trainloader = DataLoader(dataset=datasets,\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=True,\n",
    "                             drop_last=True) # drop_last: 데이터는 6만개인데 배치사이즈가 256이면 딱 안나눠지니까 나머지는 떨구는 것\n",
    "    model.train() # 이번 모델은 train에 쓰겠다\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_acc = 0.0\n",
    "\n",
    "    for i, (X, y) in enumerate (trainloader): # 데이터를 만들어주는 것\n",
    "        X = X.to(args.device)\n",
    "        y_true = y.to(args.device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        accuracy = acc(y_pred=y_pred, y_true= y_true, batch_size=args.batch_size)\n",
    "\n",
    "        model.zero_grad() # 초기화하라\n",
    "        optimizer.zero_grad() # 초기화하라\n",
    "        loss.backward() # 로스 (오류) 역전파해라\n",
    "        optimizer.step() # 옵티마이저는 가중치를 갱신하라\n",
    "\n",
    "        train_acc += accuracy\n",
    "\n",
    "    train_acc = train_acc / len(trainloader)\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "def validate(model, datasets, args):\n",
    "    valloader = DataLoader(dataset=datasets,\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=True)\n",
    "    model.eval() # 이번 모델은 val에 쓰겠다\n",
    "\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "            X = X.to(args.device)\n",
    "            y_true = y.to(args.device)\n",
    "            y_pred = model(X)\n",
    "\n",
    "            accuracy = acc(y_pred=y_pred, y_true=y_true, batch_size=args.batch_size)\n",
    "            val_acc += accuracy\n",
    "\n",
    "    val_acc = val_acc / len(valloader)\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "def training(train_datasets, val_datasets, args):\n",
    "    model = args.model\n",
    "    model.to(args.device)\n",
    "\n",
    "    print('Device : ', args.device)\n",
    "\n",
    "    loss_fn = args.loss_fn\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr) # 모델의 parameter들과 learnig rate를 참고해라\n",
    "\n",
    "    epoch_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        ts = time.time()\n",
    "\n",
    "        train_acc = train(model=model, datasets=train_datasets, optimizer=optimizer, loss_fn=loss_fn, args=args)\n",
    "        val_acc = validate(model=model, datasets=val_datasets, args=args)\n",
    "\n",
    "        te = time.time()\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        epoch_list.append(epoch)\n",
    "        print('Epoch {}, acc(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch,train_acc,val_acc,te - ts))\n",
    "\n",
    "    plt.title('ResNet')\n",
    "    plt.plot(epoch_list, train_acc_list)\n",
    "    plt.plot(epoch_list, val_acc_list)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(model.state_dict(), 'CNN.pt') # 모델의 가중치 상태를 dict형태로 저장\n",
    "\n",
    "# 모델 훈련시키기    \n",
    "    \n",
    "training(train_datasets, validation_datasets, deepcopy(args))\n",
    "\n",
    "# model = ResNet()\n",
    "# model.load_stat_dict(torch.load('CNN.pt'))를 쓰면 원래 사용하던 모델을 불러와서 이어서 쓸 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 케라스\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Activation, add, Flatten, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, width, height, 1).astype('float32')/255.0\n",
    "x_test = x_test.reshape(10000, width, height, 1).astype('float32')/255.0\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 11, 11, 64)   3200        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 6, 6, 64)     0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 6, 6, 64)     256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 6, 6, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 6, 6, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 6, 6, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 6, 6, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 6, 6, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 6, 6, 64)     0           conv2d_20[0][0]                  \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 6, 6, 64)     256         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 6, 6, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 6, 6, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 6, 6, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 6, 6, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 6, 6, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 6, 64)     0           conv2d_22[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 6, 6, 64)     256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 2, 2, 256)    147712      batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 2, 2, 256)    1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 2, 2, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 2, 2, 256)    590080      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 2, 2, 256)    1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 2, 2, 256)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 2, 2, 256)    590080      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 2, 2, 256)    0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 2, 2, 256)    1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 2, 2, 256)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 2, 2, 256)    590080      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 2, 2, 256)    1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 2, 2, 256)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 2, 2, 256)    590080      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 2, 2, 256)    0           conv2d_27[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 2,666,890\n",
      "Trainable params: 2,664,202\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def Residual_Block(x, n_ch):\n",
    "    skip_connection = x # 초기의 x\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(n_ch, kernel_size=(3,3), strides=1, padding='same')(x) # 크기 안변하게 하는 패딩이 케라스에선 same\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(n_ch, kernel_size=(3,3), strides=1, padding='same')(x)\n",
    "    x = add([x, skip_connection])\n",
    "    \n",
    "    return x\n",
    "\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = Conv2D(64, kernel_size=7, strides=2, padding='valid')(inputs) # 크기가 변해야 하므로 (반으로 줄어야 하므로) 일단 valid\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
    "x = Residual_Block(x, 64) # x = add([x, skip_connection])에 이미 x가 들어가 있기 때문에 x가 연결될 필요가 없다\n",
    "x = Residual_Block(x, 64)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, kernel_size=3, strides=2, padding='valid')(x)\n",
    "x = Residual_Block(x, 256)\n",
    "x = Residual_Block(x, 256)\n",
    "x = GlobalAveragePooling2D()(x) # 빈 괄호는 option 없는 것\n",
    "\n",
    "outputs = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=540, epochs=10, verbose = 2, validation_split=0.1) \n",
    "score = model.evaluate(x_test, y_test) # [loss, acc]\n",
    "\n",
    "print('loss: ', score[0])\n",
    "print('acc: ', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
